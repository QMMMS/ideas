# OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models

> https://arxiv.org/abs/2410.23703

离线策略评估旨在仅使用收集到的数据来估计目标策略模型的性能，而无需目标策略与现实环境之间的直接互动。然而，考虑到注释者可能不具备各种类型知识背景和相应推理的全面性，收集关于思维链推理的人类反馈可能会更具挑战性。此外，由于思维链推理涉及顺序决策过程，收集到的人类反馈量可以呈指数级增长。由于这些挑战，传统基于人类反馈的强化学习方法（RLHF）可能会面临训练效率低下和可扩展性问题。

我们提出利用知识图谱作为弱但可控的知识推理器来有效衡量通过逆倾向得分（IPS）确保 LLM 的多步骤思维链推理与多跳知识图谱轨迹的对齐。与现有的思维链评估方法不同，后者依赖于在特定知识图谱上的准确思维链定位，我们提出将知识图谱轨迹表述出来，并开发一种知识图谱策略，作为在图谱上的语言推理机制。因此，我们可以**弥合知识图谱与LLM推理形式之间的异质性**，并且表述出来的知识图谱策略可以泛化以兼容各种LLMs。

为了在LLMs中实现可控的思维链对齐，我们主要通过将LLM生成思维链推理步骤的决策过程表述为一个马尔可夫决策过程（MDP），该过程的目标是以最小的知识探索和利用达到正确的最终答案。然后，我们提出了一种离线思维链评估和对齐方法，OCEAN，它通过收集带有知识图谱反馈的离线数据样本，对来自非策略性LLMs生成的思维链进行评估。改进的知识图谱IPS方法考虑了知识图谱策略反馈的影响，该反馈使模型的思维链生成与行为策略保持一致，防止模型退化。

我们证明了KG-IPS估计量为目标策略提供了一个无偏估计，并且**通过次高斯浓度不等式建立了方差的下限**。为了能够直接优化 LLM 策略，我们利用提出的KG-IPS策略评估方法对LLM进行微调，通过梯度下降法直接最大化估计的策略值。

我们在三种类型的思维链推理任务上实证评估了优化后的LLM策略，并展示了所提出的策略优化方法的有效性。我们还观察到在评估任务上的相对性能提升，而不影响LLM的泛化能力或生成质量。

## 背景

给定提示指令q，因果语言模型 $$\pi_θ$$ 中的思维链推理过程包括在最终答案预测y之前，生成一系列推理步骤c=(c1,c2,...,c_T)的轨迹。可控的思路链生成可能具有挑战性，因为其本质在于自回归序列采样，这会在采样包含多个标记的推理步骤 $$\pi_{\theta}(c_t\mid q)$$ 时产生一个高维的动作空间。
$$
c_{t}\sim\pi_{\theta}(\cdot\mid q)=\prod_{i=1}^{t-1}\pi_{\theta}(c_{i}\mid q,c_{<i}),\quad y\sim\pi_{\theta}(\cdot\mid q)=\pi_{\theta}(y\mid q,\boldsymbol{c})\prod_{i=1}^{T}\pi_{\theta}(c_{i}\mid q,c_{<i})
$$
思路链推理可以被看作是一个马尔可夫决策过程（MDP）：

- 从指令提示q开始，LLM 依次决定并生成下一步推理路径 $$c_t$$，该路径导航直到到达目标最终答案y
- 给定LLM策略  $$\pi_θ$$ ，在时间步骤 t，每个状态 $$s_t \in \mathcal{S}$$ 包括指令提示q和之前生成的推理路径
- LLM的动作空间 $$\{1,...|\mathcal{V}|\}^{N_t}$$ 是从相同的有限词汇集V中抽样的 $$N_t$$ 个标记的序列
- LLM策略  $$\pi_θ$$ 基于当前状态采样下一步思路，即 $$a_{t}\sim\pi_{\theta}\left(a_{t}\mid s_{t}\right)$$，这是推理路径中的一个子序列，而周围的上下文由LLM确定性地生成
- 思路链中的转换是将每个推理路径连接到当前状态，即 $$s_{t+1}=[s_t,c_t]$$
- 奖励函数是评估给定状态下的每个思路，即 $$r_{t}=r(s_t,c_t)$$

尽管这种思维链的表述能够通过强化学习直接对大型语言模型进行在策略优化，但在大型语言模型中与知识图谱直接交互以收集每一步奖励在实践中可能具有挑战性，并且由于大型语言模型的非结构化生成与结构化知识图谱之间的差异，需要大量工程努力。因此，我们提出离线评估和优化与知识图谱偏好一致的目标策略。

与思维链推理不同，传统的知识图谱推理方法在第t步从图 $$\mathcal{G}=(\mathcal{E},\mathcal{V})$$的一个子集中采样一个实体-关系对，$$(r_t,e_t)$$ 该子集由当前边 $$e_{t-1}$$ 的外出边组成。
$$
\left(r_{t}, e_{t}\right) \in \left\{\left(r^{\prime}, e^{\prime}\right) \mid\left(e_{t-1}, r^{\prime}, e^{\prime}\right) \in \mathcal{G}\right\}
$$
实体 $$e_{t-1}$$ 到所有外出边的过渡可行性完全由 $$\mathcal{G}$$ 决定。知识图谱推理的过程从指令q的一个分解三元组 (*e*0,*r*1,*e*1) 开始，并通过从一个策略 *μ* 中采样来生成一个三元组链 $$\boldsymbol{h}=\left(e_{0}, r_{1},e_{1},\ldots, r_{T},e_{T}\right)$$
$$
\left(r_{t}, e_{t}\right) \sim \mu\left(\left(r_{t}, e_{t}\right) \mid e_{0}, r_{1}, e_{1}, \ldots, r_{t-1}, e_{t-1}\right)
$$
知识图谱探索的目标是在搜索步骤T结束时得出正确的答案实体。通过知识图谱探索，我们可以收集一组轨迹 $$\mathbb{H}=\left\{\boldsymbol{h}_{k}\right\}_{k=1}^{K}$$，这些轨迹用来估计一个参数概率策略 $$\mu_{\phi}$$ 作为代理模型来模拟知识图谱的偏好。

为了对齐知识图谱偏好策略  $$\mu_{\phi}$$ 与目标策略  $$\pi_θ$$ 之间的动作空间，我们利用一个小型语言模型作为 $$\mu_{\phi}$$ 的核心，并在自然语言上下文中对模型进行微调，以处理口头化的轨迹。受到将结构化知识图谱转化为自然语言查询和上下文，我们利用GPT-4模型f将每个三元组链h转化为一条思路链*c*=*f*(*h*)。口头化的知识图谱轨迹用于模拟知识图谱偏好